# 🔍 COMPAS Fairness Audit

This repository presents a comprehensive analysis of bias in the COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) algorithm. Leveraging Python tools and fairness metrics, this project explores how race may influence risk assessment scores—and reflects on the ethical and policy implications of such systems in real-world justice contexts.

---

## 📘 Contents

- `notebooks/compas_audit.ipynb`: Exploratory analysis using fairness metrics (e.g., False Positive Rate, Predictive Parity) with visualizations.
- `reports/PLP_AI_Ethics_Assignment.pdf`: Ethics reflection and policy recommendations for responsible algorithm deployment.
- `requirements.txt`: Python dependencies for reproducibility.
- `images/` *(optional)*: Visual assets supporting data findings.

---

## ⚖️ Motivation

Algorithms like COMPAS are widely used to assist decisions in the criminal justice system. However, flawed design or training can introduce systemic biases—particularly racial disparities. This project seeks to:

- Investigate the fairness of COMPAS scores across racial groups
- Quantify bias using concrete metrics
- Reflect on ethical accountability and data justice
- Propose policy frameworks for equitable AI deployment

---


## 📁 Contents

- `compas_audit.ipynb` — Data analysis, bias detection, and visual insights
- Exploratory fairness metrics across race and outcome labels
- Ethical commentary and suggestions for policy reform

## 🎯 Goal

Raise awareness about algorithmic justice, fairness tradeoffs, and the impact of opaque AI in criminal sentencing.

---


## 🛠️ Tools & Technologies

- `Python`, `Pandas`, `Matplotlib`
- `Jupyter Notebook`
- Fairness metrics inspired by academic and advocacy research

---

## 📄 Ethics Report

📥 [Download the full ethics report (PDF)](reports/PLP_AI_Ethics_Assignment.pdf)

This report includes:
- Ethical concerns surrounding algorithmic bias
- Stakeholder impacts and risks
- Recommendations for policy, transparency, and community engagement

---

## 📢 License & Attribution

This audit is an educational project submitted to the **PLP AI course** and guided by research on data ethics and fairness. Open for feedback, learning, and adaptation.

---

## 🙌 Author

**Leonard Phokane** – Driven by a commitment to equitable AI, community empowerment, and responsible innovation.

---

> “Bias isn’t just a flaw in data—it’s a reflection of the systems we choose to build and sustain.”

